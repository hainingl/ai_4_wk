{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the file path\n",
    "file_path_hsi = os.path.join(current_dir, '../data/^HSI (20250221000000000 _ 20240510000000000).csv')\n",
    "\n",
    "# Check if the file exists before reading\n",
    "if not os.path.exists(file_path_hsi):\n",
    "    raise FileNotFoundError(f\"The file {file_path_hsi} does not exist.\")\n",
    "\n",
    "df_hsi = pd.read_csv(file_path_hsi)\n",
    "print(df_hsi.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import sys\n",
    "\n",
    "# Get kernel information\n",
    "kernel_info = IPython.get_ipython().kernel\n",
    "\n",
    "# Print kernel information\n",
    "print(f\"Kernel ID: {kernel_info.session.session}\")\n",
    "print(f\"Kernel Session: {kernel_info.session}\")\n",
    "# print python version\n",
    "print(f\"Python Version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Define stock tickers\n",
    "ticker = \"AAPL\"  # Example: Apple stock\n",
    "\n",
    "# Fetch historical market data\n",
    "stock_data = yf.download(ticker, start=\"2024-01-01\", end=\"2024-02-01\")\n",
    "\n",
    "# Display first few rows\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "\n",
    "# Check if the file exists before reading\n",
    "if not os.path.exists(file_path_hsi):\n",
    "    raise FileNotFoundError(f\"The file {file_path_hsi} does not exist.\")\n",
    "\n",
    "df_hsi = pd.read_csv(file_path_hsi)\n",
    "\n",
    "# Ensure required columns are present\n",
    "required_columns = ['Date', 'Close', 'MCHI']\n",
    "missing_columns = [col for col in required_columns if col not in df_hsi.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "# Convert to numeric and clean data\n",
    "df_hsi['Close'] = pd.to_numeric(df_hsi['Close'].str.replace(',', ''), errors='coerce')\n",
    "df_hsi['MCHI'] = pd.to_numeric(df_hsi['MCHI'], errors='coerce')\n",
    "\n",
    "# Filter valid data points (keep rows where at least one of Close or MCHI is not NaN)\n",
    "valid_data = df_hsi.dropna(subset=['Close', 'MCHI'], how='all')\n",
    "valid_data = valid_data[valid_data['Date'] >= '2025-01-01'].tail(30)\n",
    "\n",
    "# Ensure there are enough valid data points\n",
    "if valid_data.empty:\n",
    "    raise ValueError(\"No valid data points found after cleaning.\")\n",
    "\n",
    "# Convert date format\n",
    "valid_data['Date'] = pd.to_datetime(valid_data['Date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Ensure date conversion was successful\n",
    "if valid_data['Date'].isnull().any():\n",
    "    raise ValueError(\"Date conversion failed for some entries.\")\n",
    "df_hsi = pd.read_csv(file_path_hsi)\n",
    "# Convert to numeric and clean data\n",
    "df_hsi['Close'] = pd.to_numeric(df_hsi['Close'].str.replace(',', ''), errors='coerce')\n",
    "df_hsi['MCHI'] = pd.to_numeric(df_hsi['MCHI'], errors='coerce')\n",
    "\n",
    "# Filter valid data points (keep rows where at least one of Close or MCHI is not NaN)\n",
    "# valid_data = df_hsi.dropna(subset=['Close', 'MCHI'], how='all').tail(50)\n",
    "\n",
    "# Convert date format\n",
    "valid_data['Date'] = pd.to_datetime(valid_data['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create plot with dual axes\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot HSI Close (left axis)\n",
    "ax1.plot(\n",
    "    valid_data['Date'], \n",
    "    valid_data['Close'], \n",
    "    label='HSI Close', \n",
    "    color='blue', \n",
    "    marker='o',\n",
    "    markersize=4,\n",
    "    linestyle='-'\n",
    ")\n",
    "\n",
    "# Plot MCHI (right axis)\n",
    "ax2.plot(\n",
    "    valid_data['Date'], \n",
    "    valid_data['MCHI'], \n",
    "    label='MCHI', \n",
    "    color='orange', \n",
    "    marker='s',\n",
    "    markersize=4,\n",
    "    linestyle='--'\n",
    ")\n",
    "\n",
    "# Configure plot appearance\n",
    "# Show every date on the x-axis\n",
    "ax1.set_xticks(valid_data['Date'])  # Set ticks for all dates\n",
    "ax1.set_xticklabels(valid_data['Date'], rotation=45, ha='right')  # Label all dates\n",
    "\n",
    "ax1.set_ylabel('HSI Value', color='blue')\n",
    "ax2.set_ylabel('MCHI Value', color='orange')\n",
    "plt.title('HSI and MCHI Closing Prices (Valid Data Points Only)')\n",
    "\n",
    "# Add combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure 'Close' and 'MCHI' columns are of string type before using .str accessor\n",
    "df_hsi['Close'] = df_hsi['Close'].astype(str)\n",
    "df_hsi['MCHI'] = df_hsi['MCHI'].astype(str)\n",
    "\n",
    "# Convert to numeric and clean data\n",
    "df_hsi['Close'] = pd.to_numeric(df_hsi['Close'].str.replace(',', ''), errors='coerce')\n",
    "df_hsi['MCHI'] = pd.to_numeric(df_hsi['MCHI'], errors='coerce')\n",
    "\n",
    "# Filter valid data points where both Close and MCHI have values\n",
    "valid_data = df_hsi.dropna(subset=['Close', 'MCHI'])\n",
    "\n",
    "# Calculate daily returns (gain/loss)\n",
    "valid_data['HSI_Return'] = valid_data['Close'].pct_change()\n",
    "valid_data['MCHI_Return'] = valid_data['MCHI'].pct_change()\n",
    "\n",
    "# Shift MCHI returns to align with next day's HSI returns\n",
    "valid_data['MCHI_Return_Shifted'] = valid_data['MCHI_Return'].shift(-1)\n",
    "\n",
    "# Drop rows with NaN values after shifting\n",
    "valid_data = valid_data.dropna(subset=['HSI_Return', 'MCHI_Return_Shifted'])\n",
    "\n",
    "# Calculate cross-correlation\n",
    "cross_corr = valid_data['HSI_Return'].corr(valid_data['MCHI_Return_Shifted'])\n",
    "\n",
    "# Calculate co-movement statistics\n",
    "same_direction = (\n",
    "    (valid_data['HSI_Return'] > 0) & (valid_data['MCHI_Return_Shifted'] > 0) |\n",
    "    (valid_data['HSI_Return'] < 0) & (valid_data['MCHI_Return_Shifted'] < 0)\n",
    ").sum()\n",
    "\n",
    "opposite_direction = (\n",
    "    (valid_data['HSI_Return'] > 0) & (valid_data['MCHI_Return_Shifted'] < 0) |\n",
    "    (valid_data['HSI_Return'] < 0) & (valid_data['MCHI_Return_Shifted'] > 0)\n",
    ").sum()\n",
    "\n",
    "total_days = len(valid_data)\n",
    "\n",
    "# Print results\n",
    "print(f\"Cross-Correlation (MCHI predicts HSI next day): {cross_corr:.2f}\")\n",
    "print(f\"\\nCo-Movement Statistics:\")\n",
    "print(f\"Days MCHI and HSI moved in the same direction: {same_direction} ({same_direction/total_days:.1%})\")\n",
    "print(f\"Days MCHI and HSI moved in opposite directions: {opposite_direction} ({opposite_direction/total_days:.1%})\")\n",
    "print(f\"Total days analyzed: {total_days}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schwab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_display_csv(file_path: str, num_rows: int = 10) -> None:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ignore the first row\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    # Drop empty rows\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Remove rows where the second column has values NaN, 'Description', 'nan', or '--'\n",
    "    df = df[~df.iloc[:, 1].isin([pd.NA, 'Description', 'nan', '--'])]\n",
    "    df = df.dropna(subset=[df.columns[1]])\n",
    "    \n",
    "    # Select the first 3 columns, the 7th column, and the last one\n",
    "    selected_columns = df.iloc[:, [0, 1, 2, 6, -1]].copy()\n",
    "    \n",
    "    # Rename the columns\n",
    "    selected_columns.columns = ['Symbol', 'Detail', 'Holding', 'Mkt Value', 'Type']\n",
    "    \n",
    "    # Remove '$' sign and commas, then convert 'Mkt Value' to float\n",
    "    selected_columns.loc[:, 'Mkt Value'] = selected_columns['Mkt Value'].replace('[\\$,]', '', regex=True).replace(',', '', regex=True).astype(float)\n",
    "    \n",
    "    # Convert 'Holding' to float\n",
    "    selected_columns.loc[:, 'Holding'] = selected_columns['Holding'].replace(',', '', regex=True).astype(float)\n",
    "    \n",
    "    # Group by 'Symbol' and sum 'Holding' and 'Mkt Value'\n",
    "    grouped = selected_columns.groupby('Symbol').agg({\n",
    "        'Detail': 'first',\n",
    "        'Holding': 'sum',\n",
    "        'Mkt Value': 'sum',\n",
    "        'Type': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Round 'Holding' and 'Mkt Value' to int\n",
    "    grouped['Holding'] = grouped['Holding'].round().astype(int)\n",
    "    grouped['Mkt Value'] = grouped['Mkt Value'].round().astype(int)\n",
    "    \n",
    "    # Sort by 'Mkt Value' in descending order\n",
    "    grouped = grouped.sort_values(by='Mkt Value', ascending=False)\n",
    "    \n",
    "    # Display the grouped columns as a Markdown table\n",
    "    print(grouped.head(num_rows).to_markdown(index=False))\n",
    "\n",
    "# Example usage\n",
    "file_path = '/Users/hliu/Downloads/All-Accounts-Positions-2025-02-07-164814.csv'\n",
    "load_and_display_csv(file_path, num_rows=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_and_display_csv(file_path, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create an HTML block with the provided content\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>SPY Stock Chart</title>\n",
    "    <script>\n",
    "        // Highcharts library\n",
    "        !function(a){\"object\"==typeof module&&module.exports?module.exports=a:a(Highcharts)}(function(a){a.createElement(\"link\",{href:\"https://code.highcharts.com/css/highcharts.css\",rel:\"stylesheet\",type:\"text/css\"},null,document.getElementsByTagName(\"head\")[0]),a.createElement(\"script\",{src:\"https://code.highcharts.com/stock/highstock.js\"},null,document.getElementsByTagName(\"head\")[0]),a.createElement(\"script\",{src:\"https://code.highcharts.com/modules/exporting.js\"},null,document.getElementsByTagName(\"head\")[0])});\n",
    "    </script>\n",
    "</head>\n",
    "<body>\n",
    "    <div id=\"container\" style=\"height: 600px; min-width: 100%;\"></div>\n",
    "\n",
    "    <script>\n",
    "        async function fetchSPYData() {\n",
    "            const response = await fetch(\"https://query1.finance.yahoo.com/v8/finance/chart/SPY?range=1y&interval=1d\");\n",
    "            const data = await response.json();\n",
    "            const prices = data.chart.result[0].timestamp.map((timestamp, index) => {\n",
    "                return [timestamp * 1000, data.chart.result[0].indicators.quote[0].close[index]];\n",
    "            });\n",
    "\n",
    "            Highcharts.stockChart('container', {\n",
    "                rangeSelector: { selected: 1 },\n",
    "                title: { text: 'SPY Stock Price' },\n",
    "                series: [{\n",
    "                    name: 'SPY',\n",
    "                    data: prices,\n",
    "                    tooltip: { valueDecimals: 2 }\n",
    "                }]\n",
    "            });\n",
    "        }\n",
    "\n",
    "        fetchSPYData();\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Display the HTML block\n",
    "display(HTML(html_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_4_wk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
